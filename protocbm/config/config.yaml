defaults:
  - dataset: cub_200_312
  - model: protocbm

universal:
  data_path: ./data
  run_name: "unclassified"
  log_path: ./logs/${dataset.name}/${model.type}/${universal.run_name}
  log_dirname: "{datetime}-{wandb_id}"
  x_mean: [0.485, 0.456, 0.406]
  x_std: [0.229, 0.224, 0.225]
  #mode: max
  #monitor: val_c2y_acc_cls
  #early_stop_patience: 10
  #lr_patience: 5
  mode: min
  monitor: val_loss
  early_stop_patience: 40
  lr_patience: 20

optimiser:
  type: adam
  lr: 0.001
  weight_decay: 0.0001

lr_scheduler:
  type: plateau
  monitor: ${universal.monitor}
  mode: ${universal.mode}
  patience: ${universal.lr_patience}
  factor: 0.1
  min_lr: 1e-08
  verbose: True
  threshold: 0.01
  cooldown: 0

trainer:
  precision: 32
  max_epochs: 100
  profiler: null

log_level: INFO

evaluation: {}
  #cas:
  #  step: 50

# Loggers
wandb:
  _target_: lightning.pytorch.loggers.WandbLogger
  name: ${model.type}-${dataset.name}-${universal.run_name}
  entity: linus-leong-yh
  project: Part3-ProtoCBM
  tags: []
  settings:
    _target_: wandb.Settings
    start_method: thread

tensorboard:
  dir: ${universal.log_path}/${universal.log_dirname}/tensorboard/
  name: protocbm

# Callbacks
callbacks:
  early_stopping:
    _target_: lightning.pytorch.callbacks.early_stopping.EarlyStopping
    monitor: ${universal.monitor}
    mode: ${universal.mode}
    patience: ${universal.early_stop_patience}
  model_checkpoint:
    _target_: lightning.pytorch.callbacks.model_checkpoint.ModelCheckpoint
    monitor: ${universal.monitor}
    mode: ${universal.mode}
    save_top_k: 1
    every_n_epochs: null
    save_on_train_epoch_end: null
    dirpath: ${universal.log_path}/${universal.log_dirname}/ckpt
  model_summary:
    _target_: lightning.pytorch.callbacks.RichModelSummary
    max_depth: 1
  rich_progress_bar:
    _target_: lightning.pytorch.callbacks.RichProgressBar
